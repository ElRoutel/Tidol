import sys
import os
import json
import warnings
import traceback
import gc
from faster_whisper import WhisperModel

# Suppress warnings
warnings.filterwarnings("ignore")
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

def format_timestamp(seconds):
    """Converts seconds to [MM:SS.xx] format for LRC."""
    minutes = int(seconds // 60)
    remaining_seconds = seconds % 60
    return f"[{minutes:02d}:{remaining_seconds:05.2f}]"

def get_audio_duration(audio_path):
    """Get audio duration in seconds."""
    try:
        import librosa
        duration = librosa.get_duration(path=audio_path)
        return duration
    except:
        return None

def generate_lrc(audio_path, output_path):
    model = None
    try:
        # Check audio duration to prevent memory issues
        duration = get_audio_duration(audio_path)
        if duration and duration > 600:  # 10 minutes max
            raise Exception(f"Audio too long ({duration:.0f}s). Maximum 10 minutes supported.")
        
        # Check for GPU
        import torch
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # Use int8 for CPU to save memory, float16 for GPU
        if device == "cuda":
            compute_type = "float16"
        else:
            compute_type = "int8"
        
        # Force int8 for large models to fit in 8GB VRAM
        if device == "cuda":
            compute_type = "int8"  # int8 uses ~50% less VRAM than float16
        else:
            compute_type = "int8"
        
        print(json.dumps({"status": "progress", "message": f"Loading Whisper (large-v2) on {device}..."}), flush=True)

        # Use 'large-v2' model with int8 for best accuracy in 8GB VRAM
        # tiny = ~1GB, small = ~2GB, medium = ~5GB, large-v2 (int8) = ~6-7GB
        model = WhisperModel("large-v2", device=device, compute_type=compute_type, num_workers=1)

        print(json.dumps({"status": "progress", "message": "Transcribing audio..."}), flush=True)
        
        # Increase beam_size for better accuracy with medium model
        segments, info = model.transcribe(
            audio_path, 
            beam_size=5,  # Increased from 3 for better accuracy
            vad_filter=True,  # Voice Activity Detection to skip silence
            vad_parameters=dict(min_silence_duration_ms=500)
        )

        lrc_lines = []
        
        # Header
        lrc_lines.append("[00:00.00]ðŸŽµ (Lyrics generated by Spectra VOXW)")

        segment_count = 0
        for segment in segments:
            start_time = format_timestamp(segment.start)
            text = segment.text.strip()
            if text:  # Only add non-empty segments
                lrc_lines.append(f"{start_time}{text}")
                segment_count += 1

        if segment_count == 0:
            # Create a minimal LRC file instead of erroring
            lrc_lines.append("[00:00.00]ðŸŽµ (Instrumental - No vocals detected)")
            print(json.dumps({
                "status": "warning", 
                "message": "No vocals detected, song may be instrumental",
                "segments": 0
            }), flush=True)

        # Write LRC file
        with open(output_path, "w", encoding="utf-8") as f:
            f.write("\n".join(lrc_lines))

        print(json.dumps({
            "status": "success", 
            "lrc_path": output_path,
            "language": info.language,
            "language_probability": float(info.language_probability),
            "segments": segment_count
        }), flush=True)

    except MemoryError as e:
        error_msg = "Out of memory. Try a shorter audio file or close other applications."
        print(json.dumps({"status": "error", "message": error_msg}), file=sys.stderr, flush=True)
        sys.exit(1)
    except Exception as e:
        error_msg = f"{str(e)}\n{traceback.format_exc()}"
        print(json.dumps({"status": "error", "message": error_msg}), file=sys.stderr, flush=True)
        sys.exit(1)
    finally:
        # Clean up memory
        if model is not None:
            del model
        gc.collect()
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except:
            pass

if __name__ == "__main__":
    if len(sys.argv) < 3:
        print(json.dumps({"status": "error", "message": "Usage: python voxw.py <audio_file> <output_lrc>"}))
        sys.exit(1)
    
    try:
        audio_file = sys.argv[1]
        output_lrc = sys.argv[2]
        
        if not os.path.exists(audio_file):
            print(json.dumps({"status": "error", "message": f"Audio file not found: {audio_file}"}))
            sys.exit(1)
        
        generate_lrc(audio_file, output_lrc)
    except Exception as e:
        error_msg = f"{str(e)}\n{traceback.format_exc()}"
        print(json.dumps({"status": "error", "message": error_msg}), file=sys.stderr, flush=True)
        sys.exit(1)
